{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f8d0ee-8e87-45d7-80f8-2a13cfd97104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/data/maryamsana_98/DiffCodec-Controlling-Latent-Diffusion-for-Perceptual-Video-Compression/controlnet/softsplat.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/data/maryamsana_98/DiffCodec-Controlling-Latent-Diffusion-for-Perceptual-Video-Compression/controlnet/softsplat.py:359: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(self, tenOutgrad):\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "if not hasattr(huggingface_hub, \"cached_download\"):\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    huggingface_hub.cached_download = hf_hub_download\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from safetensors.torch import load_file\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# your classes\n",
    "from controlnet.flownet import DualFlowControlNet\n",
    "from pipeline import StableDiffusionDualFlowControlNetPipeline\n",
    "from controlnet.utils import load_controls_and_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5432bc2-a6a8-4118-80d2-2984809b4dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['feature_extractor.wrapper.0.metric_net.0.weight', 'feature_extractor.wrapper.0.metric_net.0.bias', 'feature_extractor.wrapper.0.metric_net.2.weight', 'feature_extractor.wrapper.0.metric_net.2.bias', 'feature_extractor.wrapper.1.metric_net.0.weight', 'feature_extractor.wrapper.1.metric_net.0.bias', 'feature_extractor.wrapper.1.metric_net.2.weight', 'feature_extractor.wrapper.1.metric_net.2.bias', 'feature_extractor.wrapper.2.metric_net.0.weight', 'feature_extractor.wrapper.2.metric_net.0.bias', 'feature_extractor.wrapper.2.metric_net.2.weight', 'feature_extractor.wrapper.2.metric_net.2.bias', 'feature_extractor.wrapper.3.metric_net.0.weight', 'feature_extractor.wrapper.3.metric_net.0.bias', 'feature_extractor.wrapper.3.metric_net.2.weight', 'feature_extractor.wrapper.3.metric_net.2.bias', 'feature_extractor.zero_convs.0.weight', 'feature_extractor.zero_convs.1.weight', 'feature_extractor.zero_convs.2.weight', 'feature_extractor.zero_convs.3.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Load models (aligned SD-1.5)\n",
    "# ---------------------------\n",
    "dtype = torch.float32\n",
    "base = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(base, subfolder=\"vae\", torch_dtype=dtype)\n",
    "unet = UNet2DConditionModel.from_pretrained(base, subfolder=\"unet\", torch_dtype=dtype)\n",
    "text_encoder = CLIPTextModel.from_pretrained(base, subfolder=\"text_encoder\", torch_dtype=dtype)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(base, subfolder=\"tokenizer\")\n",
    "scheduler = UniPCMultistepScheduler.from_pretrained(base, subfolder=\"scheduler\")\n",
    "\n",
    "# --- ControlNet: load your subclass weights ---\n",
    "controlnet = DualFlowControlNet(\n",
    "    block_out_channels=tuple(unet.config.block_out_channels),     # (320, 640, 1280, 1280)\n",
    "    layers_per_block=2,\n",
    "    cross_attention_dim=768,   \n",
    " )\n",
    "# controlnet.load_state_dict(torch.load(\"path/to/controlnet.safetensors\" or \".pth\", map_location=\"cpu\"))\n",
    "\n",
    "# sanity: cross-attn dims must match (768 for SD1.x)\n",
    "assert unet.config.cross_attention_dim == text_encoder.config.hidden_size == 768\n",
    "if hasattr(controlnet, \"config\") and hasattr(controlnet.config, \"cross_attention_dim\"):\n",
    "    assert controlnet.config.cross_attention_dim == 768, f\"ControlNet CAD={controlnet.config.cross_attention_dim}\"\n",
    "\n",
    "ckpt = load_file('experiments/controlnet/checkpoint-153000/controlnet/diffusion_pytorch_model.safetensors')\n",
    "model_state = controlnet.state_dict()\n",
    "\n",
    "# Filter only matching keys with same shape\n",
    "filtered_state_dict = {\n",
    "    k: v for k, v in ckpt.items()\n",
    "    if k in model_state and v.shape == model_state[k].shape\n",
    "}\n",
    "\n",
    "# Load compatible weights\n",
    "# model.load_state_dict(filtered_state_dict, strict=False)\n",
    "\n",
    "controlnet.load_state_dict(filtered_state_dict ,strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a2b8cd-e153-4496-b339-d1bbc9d182a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_checker = None\n",
    "feature_extractor = None\n",
    "\n",
    "# ---------------------------\n",
    "# Build pipeline\n",
    "# ---------------------------\n",
    "pipe = StableDiffusionDualFlowControlNetPipeline(\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet,\n",
    "    controlnet=controlnet,\n",
    "    scheduler=scheduler,\n",
    "    safety_checker=safety_checker,\n",
    "    feature_extractor=feature_extractor,\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15989422-2a4b-4fa4-b3f0-5a2208dd3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data:\n",
    "device = pipe.device\n",
    "\n",
    "local_conditions = []\n",
    "flow_conditions = []\n",
    "prompts = [\"A beautiful blonde girl smiling with pink lipstick with black background\",\n",
    "           \"A Yacht with a red flag ,sailing in front of the Bosphorus in Istanbul , and bridge with cars is in the background.\" , \n",
    "           \"A German shepherd shakes off water in the middle of a forest trail\",\n",
    "           \"Honeybees hover among blooming purple flowers\"]\n",
    "\n",
    "videos = ['Beauty', 'Bosphorus', 'ShakeNDry', 'HoneyBee']\n",
    "for video in videos:\n",
    "    local,flow = load_controls_and_flows(\n",
    "    f'data/{video}/images/frame_0000.png',\n",
    "    f'data/{video}/images/frame_0004.png',\n",
    "    f'data/{video}/optical_flow/optical_flow_gop_4_raft/flow_0000_0003.flo',\n",
    "    f'data/{video}/optical_flow_bwd/optical_flow_gop_4_raft/flow_0004_0003.flo',\n",
    "    size=(512, 512),\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "    local_conditions.append(local) \n",
    "    flow_conditions.append(flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3b9e5e-ab3a-4c1b-be2c-945c3e1a5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del vae, tokenizer,unet, controlnet\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f42cef4-7705-4e13-9fac-5d7fe8a0fad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                  | 0/40 [00:00<?, ?it/s]/data/maryamsana_98/DiffCodec-Controlling-Latent-Diffusion-for-Perceptual-Video-Compression/controlnet/flownet.py:68: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 40/40 [00:16<00:00,  2.37it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 40/40 [00:14<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "pipe.enable_freeu(s1=0.9, s2=0.2, b1=1.2, b2=1.4)\n",
    "images = []\n",
    "for i in range(2):\n",
    "    # with inference_ctx:\n",
    "    out = pipe(\n",
    "        prompt=prompts[i],\n",
    "        controlnet_cond=local_conditions[i],   # [1,6,512,512]\n",
    "        flow_cond=flow_conditions[i],        # [1,4,512,512]\n",
    "        height=512,\n",
    "        width=512,\n",
    "        num_inference_steps=40,\n",
    "        guidance_scale=4.5,\n",
    "        negative_prompt=None,\n",
    "        num_images_per_prompt=2,\n",
    "        controlnet_conditioning_scale=1.7,\n",
    "        guess_mode=False,\n",
    "        output_type=\"pil\",\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    images.append(out.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b41f231f-599f-4fc9-a857-ab60d174773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_logs = []\n",
    "spacing = 20\n",
    "img_size = 512\n",
    "\n",
    "for i, video in enumerate(videos):\n",
    "    pil0 = Image.open(f\"data/{video}/images/frame_0000.png\").convert(\"RGB\").resize((img_size, img_size))\n",
    "    pil4 = Image.open(f\"data/{video}/images/frame_0004.png\").convert(\"RGB\").resize((img_size, img_size))\n",
    "    gt   = Image.open(f\"data/{video}/images/frame_0003.png\").convert(\"RGB\").resize((img_size, img_size))\n",
    "    \n",
    "    # 3 predictions\n",
    "    pred1 = images[i][0]                       # pipeline pred 1\n",
    "    pred2 = images[i][1]  # pipeline pred 2\n",
    "    pred3 = Image.open(f\"benchmark_results/preds_gop4_q4/{video}/im00003_pred.png\").convert(\"RGB\").resize((img_size, img_size))\n",
    "    \n",
    "    preds = [pred1.resize((img_size,img_size)), \n",
    "             pred2.resize((img_size,img_size)), \n",
    "             pred3.resize((img_size,img_size))]\n",
    "    labels = [\"Pred 1 - Pipeline\", \"Pred 2 - Pipeline\", \"Pred 3 - UniControl\"]\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {}\n",
    "    for j, p in enumerate(preds):\n",
    "        pred_tensor = torch.from_numpy(np.array(p).transpose(2,0,1)).float() / 255.0\n",
    "        gt_tensor   = torch.from_numpy(np.array(gt).transpose(2,0,1)).float() / 255.0\n",
    "        pred_tensor = pred_tensor.unsqueeze(0)\n",
    "        gt_tensor   = gt_tensor.unsqueeze(0)\n",
    "        \n",
    "        ms_ssim_val = ms_ssim(pred_tensor, gt_tensor, data_range=1.0, size_average=True).item()\n",
    "        mse = F.mse_loss(pred_tensor, gt_tensor).item()\n",
    "        psnr_val = 10 * np.log10(1.0 / mse) if mse != 0 else float('inf')\n",
    "        \n",
    "        metrics[labels[j]] = {\"MS-SSIM\": ms_ssim_val, \"PSNR\": psnr_val}\n",
    "    \n",
    "    # --- Plot with matplotlib ---\n",
    "    all_imgs = preds + [gt]\n",
    "    titles = [f\"{labels[j]}\\nPSNR: {metrics[labels[j]]['PSNR']:.3f}, MS-SSIM: {metrics[labels[j]]['MS-SSIM']:.3f}\" \n",
    "                                       for j in range(len(preds))] + [\"Ground Truth\"]\n",
    "    \n",
    "    ncols = len(all_imgs)\n",
    "    fig, axs = plt.subplots(1, ncols, figsize=(4*ncols, 6))\n",
    "    \n",
    "    for ax, img, title in zip(axs, all_imgs, titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = f\"benchmark_results/{video}_comparison_free_u.svg\"\n",
    "    plt.savefig(save_path, format=\"svg\", dpi=800, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Log\n",
    "    image_logs.append({\n",
    "        \"video\": video,\n",
    "        \"preds\": preds,\n",
    "        \"ground_truth\": gt,\n",
    "        \"metrics\": metrics\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b705b-2815-40db-b55c-2090684d71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype  = torch.float32\n",
    "\n",
    "# --- Load the pretrained VAE ---\n",
    "base = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "vae = AutoencoderKL.from_pretrained(base, subfolder=\"vae\", torch_dtype=dtype).to(device)\n",
    "\n",
    "# --- Load and preprocess image ---\n",
    "path = \"data/Beauty/images/frame_0000.png\"\n",
    "image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((512, 512)),   # match training resolution\n",
    "    T.ToTensor(),           # [0,1]\n",
    "    T.Normalize([0.5], [0.5]),  # [-1,1]\n",
    "])\n",
    "img_tensor = transform(image).unsqueeze(0).to(device, dtype=dtype)  # [1,3,512,512]\n",
    "\n",
    "# --- Encode with VAE ---\n",
    "with torch.no_grad():\n",
    "    posterior = vae.encode(img_tensor).latent_dist\n",
    "    latents = posterior.sample() * vae.config.scaling_factor  # [1,4,64,64]\n",
    "\n",
    "print(\"Latent shape:\", latents.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ff14a-b87f-4c6d-b0bb-66c6eea326c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = latents * pipe.scheduler.init_noise_sigma\n",
    "out = pipe(prompt, sixch, flow4,guidance_scale=4 ,  controlnet_conditioning_scale=1.85,latents =latents )\n",
    "out[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e8c0e-2926-4259-96d4-9ae961e3dd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ( diffusers)",
   "language": "python",
   "name": "diffusers_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
