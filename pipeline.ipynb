{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f8d0ee-8e87-45d7-80f8-2a13cfd97104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/data/maryamsana_98/DiffCodec-Controlling-Latent-Diffusion-for-Perceptual-Video-Compression/softsplat.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "/data/maryamsana_98/DiffCodec-Controlling-Latent-Diffusion-for-Perceptual-Video-Compression/softsplat.py:359: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(self, tenOutgrad):\n"
     ]
    }
   ],
   "source": [
    "# import huggingface_hub\n",
    "# if not hasattr(huggingface_hub, \"cached_download\"):\n",
    "#     from huggingface_hub import hf_hub_download\n",
    "#     huggingface_hub.cached_download = hf_hub_download\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "from safetensors.torch import load_file\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# your classes\n",
    "from flownet import DualFlowControlNet\n",
    "from pipeline import StableDiffusionDualFlowControlNetPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f615412a-07e6-4f1b-a0d6-71b881be99e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Helpers: image & .flo loaders\n",
    "# ---------------------------\n",
    "def read_flo(path: str) -> np.ndarray:\n",
    "    \"\"\"Middlebury .flo → [H,W,2] float32 (pixel units).\"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        magic = np.fromfile(f, np.float32, 1)[0]\n",
    "        if magic != 202021.25:\n",
    "            raise ValueError(f\"Invalid .flo file: {path} (magic={magic})\")\n",
    "        w = int(np.fromfile(f, np.int32, 1)[0])\n",
    "        h = int(np.fromfile(f, np.int32, 1)[0])\n",
    "        data = np.fromfile(f, np.float32, 2 * w * h).reshape(h, w, 2)\n",
    "    return data\n",
    "\n",
    "def resize_flow_to(flow_hw2: np.ndarray, target_h: int, target_w: int) -> torch.Tensor:\n",
    "    \"\"\"Resize flow with bilinear and scale vectors to remain in pixel units.\"\"\"\n",
    "    ft = torch.from_numpy(flow_hw2).permute(2, 0, 1).unsqueeze(0)  # [1,2,H,W]\n",
    "    _, _, H, W = ft.shape\n",
    "    ft = F.interpolate(ft, size=(target_h, target_w), mode=\"bilinear\", align_corners=True)\n",
    "    ft[:, 0] *= (target_w / max(W, 1))\n",
    "    ft[:, 1] *= (target_h / max(H, 1))\n",
    "    return ft  # [1,2,target_h,target_w]\n",
    "\n",
    "def load_pair_to_sixch(path0, path1, size=(512, 512)) -> torch.Tensor:\n",
    "    \"\"\"Two RGB images → [1,6,H,W] in [0,1].\"\"\"\n",
    "    def load_rgb(p):\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        if size is not None:\n",
    "            img = img.resize(size, Image.BICUBIC)\n",
    "        return TF.to_tensor(img)  # [3,H,W], float32\n",
    "    a = load_rgb(path0)\n",
    "    b = load_rgb(path1)\n",
    "    return torch.cat([a, b], dim=0).unsqueeze(0)  # [1,6,H,W]\n",
    "\n",
    "def load_controls_and_flows(\n",
    "    img0_path, img1_path, fwd_flo_path, bwd_flo_path, size=(512, 512), device=\"cuda\", dtype=torch.float32\n",
    "):\n",
    "    H, W = size\n",
    "    sixch = load_pair_to_sixch(img0_path, img1_path, size=size).to(device=device, dtype=dtype)  # [1,6,H,W]\n",
    "\n",
    "    fwd = read_flo(fwd_flo_path)\n",
    "    bwd = read_flo(bwd_flo_path)\n",
    "    fwd_t = resize_flow_to(fwd, H, W)\n",
    "    bwd_t = resize_flow_to(bwd, H, W)\n",
    "    flow4 = torch.cat([fwd_t, bwd_t], dim=1).to(device=device, dtype=dtype)  # [1,4,H,W]\n",
    "    return sixch, flow4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15989422-2a4b-4fa4-b3f0-5a2208dd3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype  = torch.float32 \n",
    "# ---------------------------\n",
    "# Load controls\n",
    "# ---------------------------\n",
    "sixch, flow4 = load_controls_and_flows(\n",
    "    \"data/Beauty/images/frame_0000.png\",\n",
    "    \"data/Beauty/images/frame_0002.png\",\n",
    "    \"data/Beauty/optical_flow/optical_flow_gop_2_raft/flow_0000_0001.flo\",\n",
    "    \"data/Beauty/optical_flow_bwd/optical_flow_gop_2_raft/flow_0002_0001.flo\",\n",
    "    size=(512, 512),\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90ab2630-584c-490b-8a7d-5fef4d01f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"stabilityai/sdxl-vae\",     # official HF repo\n",
    "    torch_dtype=dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5432bc2-a6a8-4118-80d2-2984809b4dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Load models (aligned SD-1.5)\n",
    "# ---------------------------\n",
    "base = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "\n",
    "# vae = AutoencoderKL.from_pretrained(base, subfolder=\"vae\", torch_dtype=dtype)\n",
    "unet = UNet2DConditionModel.from_pretrained(base, subfolder=\"unet\", torch_dtype=dtype)\n",
    "text_encoder = CLIPTextModel.from_pretrained(base, subfolder=\"text_encoder\", torch_dtype=dtype)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(base, subfolder=\"tokenizer\")\n",
    "scheduler = UniPCMultistepScheduler.from_pretrained(base, subfolder=\"scheduler\")\n",
    "\n",
    "# --- ControlNet: load your subclass weights ---\n",
    "controlnet = DualFlowControlNet(\n",
    "    block_out_channels=tuple(unet.config.block_out_channels),     # (320, 640, 1280, 1280)\n",
    "    layers_per_block=2,\n",
    "    cross_attention_dim=768,   \n",
    " )\n",
    "# controlnet.load_state_dict(torch.load(\"path/to/controlnet.safetensors\" or \".pth\", map_location=\"cpu\"))\n",
    "\n",
    "# sanity: cross-attn dims must match (768 for SD1.x)\n",
    "assert unet.config.cross_attention_dim == text_encoder.config.hidden_size == 768\n",
    "if hasattr(controlnet, \"config\") and hasattr(controlnet.config, \"cross_attention_dim\"):\n",
    "    assert controlnet.config.cross_attention_dim == 768, f\"ControlNet CAD={controlnet.config.cross_attention_dim}\"\n",
    "\n",
    "ckpt = load_file('experiments/controlnet/checkpoint-11500/controlnet/diffusion_pytorch_model.safetensors')\n",
    "controlnet.load_state_dict(ckpt,strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b67bb96-b2c2-4116-babe-0506978de367",
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_checker = None\n",
    "feature_extractor = None\n",
    "\n",
    "# ---------------------------\n",
    "# Build pipeline\n",
    "# ---------------------------\n",
    "pipe = StableDiffusionDualFlowControlNetPipeline(\n",
    "    vae=vae,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    unet=unet,\n",
    "    controlnet=controlnet,\n",
    "    scheduler=scheduler,\n",
    "    safety_checker=safety_checker,\n",
    "    feature_extractor=feature_extractor,\n",
    ")\n",
    "# pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a84bcc1-f27f-4d63-afc0-1fa86f2e35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a pretty girl smiling , has pink lipstick and is infront of black background\"\n",
    "g = torch.Generator(device=device).manual_seed(42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b9e5e-ab3a-4c1b-be2c-945c3e1a5dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del vae, tokenizer,unet, controlnet\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa626a-c2fb-4cc3-82db-ef2d18186f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b705b-2815-40db-b55c-2090684d71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype  = torch.float32\n",
    "\n",
    "# --- Load the pretrained VAE ---\n",
    "base = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "vae = AutoencoderKL.from_pretrained(base, subfolder=\"vae\", torch_dtype=dtype).to(device)\n",
    "\n",
    "# --- Load and preprocess image ---\n",
    "path = \"data/Beauty/images/frame_0000.png\"\n",
    "image = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((512, 512)),   # match training resolution\n",
    "    T.ToTensor(),           # [0,1]\n",
    "    T.Normalize([0.5], [0.5]),  # [-1,1]\n",
    "])\n",
    "img_tensor = transform(image).unsqueeze(0).to(device, dtype=dtype)  # [1,3,512,512]\n",
    "\n",
    "# --- Encode with VAE ---\n",
    "with torch.no_grad():\n",
    "    posterior = vae.encode(img_tensor).latent_dist\n",
    "    latents = posterior.sample() * vae.config.scaling_factor  # [1,4,64,64]\n",
    "\n",
    "print(\"Latent shape:\", latents.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e47de34-0a85-48b1-9d23-19601984c85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msixch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow4\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/maryamsana_98/DiffCodec-Controlling-Latent-Diffusion-for-Perceptual-Video-Compression/pipeline.py:341\u001b[0m, in \u001b[0;36mStableDiffusionDualFlowControlNetPipeline.__call__\u001b[0;34m(self, prompt, controlnet_cond, flow_cond, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, cross_attention_kwargs, controlnet_conditioning_scale, guess_mode, control_guidance_start, control_guidance_end, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m cond_scale \u001b[38;5;241m=\u001b[39m cond_base_scale \u001b[38;5;241m*\u001b[39m controlnet_keep[i]\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# ---- ControlNet forward -> residuals ----\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m down_block_res_samples, mid_block_res_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrolnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_cond_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflow_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflow_cond_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconditioning_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# If guess_mode+CFG, expand to unconditional half with zeros (official behavior)\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m guess_mode \u001b[38;5;129;01mand\u001b[39;00m do_cfg:\n",
      "File \u001b[0;32m/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/maryamsana_98/DiffCodec-Controlling-Latent-Diffusion-for-Perceptual-Video-Compression/flownet.py:260\u001b[0m, in \u001b[0;36mDualFlowControlNet.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, controlnet_cond, flow_cond, conditioning_scale, guess_mode, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embedding(t_emb)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# ---- pyramid control (warped & projected) ----\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m P64, P32, P16, P08 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrolnet_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m P64\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m P32\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m P16\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m P08\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# ---- mirrored ControlNet path with injections ----\u001b[39;00m\n",
      "File \u001b[0;32m/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/maryamsana_98/DiffCodec-Controlling-Latent-Diffusion-for-Perceptual-Video-Compression/flownet.py:174\u001b[0m, in \u001b[0;36mBi_Dir_FeatureExtractor.forward\u001b[0;34m(self, local_conditions, flow)\u001b[0m\n\u001b[1;32m    171\u001b[0m flow \u001b[38;5;241m=\u001b[39m resize_and_normalize_flow_batched(flow_fwd, target_h\u001b[38;5;241m=\u001b[39mflow_res[idx], target_w\u001b[38;5;241m=\u001b[39mflow_res[idx])\n\u001b[1;32m    172\u001b[0m flow_b \u001b[38;5;241m=\u001b[39m resize_and_normalize_flow_batched(flow_bwd, target_h\u001b[38;5;241m=\u001b[39mflow_res[idx], target_w\u001b[38;5;241m=\u001b[39mflow_res[idx])\n\u001b[0;32m--> 174\u001b[0m wrapped_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m wrapped_last \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper(last_features,flow_b)\n\u001b[1;32m    177\u001b[0m local_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([wrapped_first,wrapped_last],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/maryamsana_98/DiffCodec-Controlling-Latent-Diffusion-for-Perceptual-Video-Compression/flownet.py:61\u001b[0m, in \u001b[0;36mFeatureWarperSoftsplat.forward\u001b[0;34m(self, feat_ref, flow, mask)\u001b[0m\n\u001b[1;32m     57\u001b[0m     metric \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(flow[:, :\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# shape: [B, 1, H, W]\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# print('metric', metric.shape)\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m warped \u001b[38;5;241m=\u001b[39m \u001b[43msoftsplat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenIn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeat_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenFlow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenMetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrMode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msoft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     66\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m      warped \u001b[38;5;241m=\u001b[39m warped\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mmask)\n",
      "File \u001b[0;32m/data/maryamsana_98/DiffCodec-Controlling-Latent-Diffusion-for-Perceptual-Video-Compression/softsplat.py:251\u001b[0m, in \u001b[0;36msoftsplat\u001b[0;34m(tenIn, tenFlow, tenMetric, strMode)\u001b[0m\n\u001b[1;32m    247\u001b[0m     tenIn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([tenIn \u001b[38;5;241m*\u001b[39m tenMetric\u001b[38;5;241m.\u001b[39mexp(), tenMetric\u001b[38;5;241m.\u001b[39mexp()], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# end\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m tenOut \u001b[38;5;241m=\u001b[39m \u001b[43msoftsplat_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtenIn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtenFlow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strMode\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    254\u001b[0m     tenNormalize \u001b[38;5;241m=\u001b[39m tenOut[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :, :]\n",
      "File \u001b[0;32m/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m/data/maryamsana_98/anaconda3/envs/diffusers_env/lib/python3.10/site-packages/torch/amp/autocast_mode.py:466\u001b[0m, in \u001b[0;36mcustom_fwd.<locals>.decorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fwd(\n\u001b[1;32m    462\u001b[0m             \u001b[38;5;241m*\u001b[39m_cast(args, device_type, cast_inputs),\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_cast(kwargs, device_type, cast_inputs),\n\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/maryamsana_98/DiffCodec-Controlling-Latent-Diffusion-for-Perceptual-Video-Compression/softsplat.py:348\u001b[0m, in \u001b[0;36msoftsplat_func.forward\u001b[0;34m(self, tenIn, tenFlow)\u001b[0m\n\u001b[1;32m    284\u001b[0m     cuda_launch(cuda_kernel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftsplat_out\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124m        extern \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m __global__ void __launch_bounds__(512) softsplat_out(\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124m            const int n,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m         stream\u001b[38;5;241m=\u001b[39mcollections\u001b[38;5;241m.\u001b[39mnamedtuple(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStream\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mptr\u001b[39m\u001b[38;5;124m'\u001b[39m)(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mcurrent_stream()\u001b[38;5;241m.\u001b[39mcuda_stream)\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tenIn\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# end\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_for_backward(tenIn, tenFlow)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = pipe(prompt, sixch, flow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fcf48-6859-45bd-9307-14c784a40588",
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
